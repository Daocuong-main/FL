{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bZAnYwAkdAFl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-31 01:27:50.555974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-10-31 01:27:50.707872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-10-31 01:27:50.707894: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-10-31 01:27:50.736975: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-10-31 01:27:51.434170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2022-10-31 01:27:51.434247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2022-10-31 01:27:51.434255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "from time import time, localtime\n",
        "import numpy as np\n",
        "from utils import plot_graph\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import losses, metrics, optimizers\n",
        "import random\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "from pathlib import Path\n",
        "from checkpoint_manager import FileCheckpointManager\n",
        "import tensorflow_addons as tfa\n",
        "nest_asyncio.apply()\n",
        "SEED = 1337\n",
        "tf.random.set_seed(SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.CNN import create_keras_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZYYi42tYdAFp"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"GQUIC\"\n",
        "method = \"CNN\"\n",
        "client_lr = 1\n",
        "server_lr = 1\n",
        "NUM_ROUNDS = 1\n",
        "BATCH_SIZE = 1\n",
        "split = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "this_dir = Path.cwd()\n",
        "model_dir = this_dir / \"sdn_saved_models\" / experiment_name / method\n",
        "output_dir = this_dir / \"sdn_results\" / experiment_name / method\n",
        "\n",
        "if not model_dir.exists():\n",
        "    model_dir.mkdir(parents=True)\n",
        "\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir(parents=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sec_to_hours(seconds):\n",
        "    a = seconds//3600\n",
        "    b = (seconds % 3600)//60\n",
        "    c = (seconds % 3600) % 60\n",
        "    d = \"{:.0f} hours {:.0f} mins {:.0f} seconds\".format(a, b, c)\n",
        "    return d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTm_q5vvdAFm"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv(\n",
        "    '/home/onos/FL/Data Processing/GQUIC_data.csv', index_col=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_FEATURE = len(data.columns)-1\n",
        "NUM_CLASSES = len(np.unique(data['Label']))\n",
        "total_data_count = data.shape[0]\n",
        "data_per_set = int(np.floor(total_data_count/split))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = pd.read_csv(\n",
        "    '/home/onos/FL/Data Processing/GQUIC_data_test.csv', index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = test.drop('Label', axis=1).to_numpy()\n",
        "y_test = test.Label.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data.sample(frac=1).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding data from 0 to 188844 for client : client_1\n",
            "Adding data from 188844 to 377688 for client : client_2\n",
            "Adding data from 377688 to 566532 for client : client_3\n",
            "Adding data from 566532 to 755376 for client : client_4\n",
            "Adding data from 755376 to 944220 for client : client_5\n"
          ]
        }
      ],
      "source": [
        "DataFrameDict = {}\n",
        "for i in range(1, split+1):\n",
        "    client_name = \"client_\" + str(i)\n",
        "    start = data_per_set * (i-1)\n",
        "    end = data_per_set * i\n",
        "\n",
        "    print(f\"Adding data from {start} to {end} for client : {client_name}\")\n",
        "    DataFrameDict[client_name] = data[start:end]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def sklearn_to_df(sklearn_dataset):\n",
        "#     df = pd.DataFrame(sklearn_dataset.data,\n",
        "#                       columns=sklearn_dataset.feature_names)\n",
        "#     df['Label'] = pd.Series(sklearn_dataset.Label)\n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn import datasets\n",
        "# df = sklearn_to_df(datasets.load_iris())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNP4fyRtdAFm"
      },
      "source": [
        "### Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tzo3e8h1dAFn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3    463260\n",
            "2    261747\n",
            "1    169592\n",
            "0     49622\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(data['Label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "client_1\n",
            "3    92876\n",
            "2    52229\n",
            "1    33703\n",
            "0    10036\n",
            "Name: Label, dtype: int64\n",
            "client_2\n",
            "3    92495\n",
            "2    52564\n",
            "1    33980\n",
            "0     9805\n",
            "Name: Label, dtype: int64\n",
            "client_3\n",
            "3    92618\n",
            "2    52286\n",
            "1    33954\n",
            "0     9986\n",
            "Name: Label, dtype: int64\n",
            "client_4\n",
            "3    92285\n",
            "2    52453\n",
            "1    34171\n",
            "0     9935\n",
            "Name: Label, dtype: int64\n",
            "client_5\n",
            "3    92985\n",
            "2    52215\n",
            "1    33784\n",
            "0     9860\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "for df in DataFrameDict.keys():\n",
        "    print(df)\n",
        "    print(DataFrameDict[df]['Label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpgncmHIdAFp"
      },
      "source": [
        "## Federated Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIg-Ji57dAFp"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QIx3CIRydAFq"
      },
      "outputs": [],
      "source": [
        "def make_tf_dataset(dataframe, batch_size=None):\n",
        "\n",
        "    y = dataframe.pop('Label')\n",
        "\n",
        "    # Dataset creation\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dataframe.values, y.to_frame().values))\n",
        "    dataset = dataset.shuffle(2048, seed=SEED)\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zBj7fQxFdAFq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-31 01:28:18.975111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-10-31 01:28:18.975152: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-10-31 01:28:18.975168: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (onos-virtual-machine): /proc/driver/nvidia/version does not exist\n",
            "2022-10-31 01:28:18.975516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "train_data, val_data = [], []\n",
        "for client_data in DataFrameDict.keys():\n",
        "    train_df, val_df = train_test_split(\n",
        "        DataFrameDict[client_data], test_size=0.2, random_state=SEED)\n",
        "\n",
        "    # Scaling (Standardization actually hurts performance)\n",
        "    scaler = MinMaxScaler()\n",
        "    train_features = scaler.fit_transform(train_df.drop(['Label'], axis=1))\n",
        "    val_features = scaler.transform(val_df.drop(['Label'], axis=1))\n",
        "\n",
        "    train_df[train_df.columns.difference(['Label'])] = train_features\n",
        "    val_df[val_df.columns.difference(['Label'])] = val_features\n",
        "\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE))\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP8byNHDdAFq"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rCci5e2-dAFq"
      },
      "outputs": [],
      "source": [
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, NUM_FEATURE], tf.float64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    model = create_keras_model(NUM_CLASSES)\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd99tw3RdAFr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XUqmO4HdAFs"
      },
      "source": [
        "Each time the `next` method is called, the server model is broadcast to each client using a broadcast function. For each client, one epoch of local training is performed. Each client computes the difference between the client model after training and the initial broadcast model. These model deltas are then aggregated at the server using some aggregation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "tff_train_acc = []\n",
        "tff_train_loss = []\n",
        "tff_val_acc = []\n",
        "tff_val_loss = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "x1YqmgzhdAFr"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 175, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 33, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 458, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 411, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 401, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 1, 129), found shape=(None, 129)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m iterative_process \u001b[39m=\u001b[39m tff\u001b[39m.\u001b[39;49mlearning\u001b[39m.\u001b[39;49malgorithms\u001b[39m.\u001b[39;49mbuild_weighted_fed_avg(\n\u001b[1;32m      2\u001b[0m     model_fn,\n\u001b[1;32m      3\u001b[0m     client_optimizer_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m: tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(\n\u001b[1;32m      4\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mclient_lr),\n\u001b[1;32m      5\u001b[0m     server_optimizer_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m: tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(\n\u001b[1;32m      6\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mserver_lr)\n\u001b[1;32m      7\u001b[0m )\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg.py:227\u001b[0m, in \u001b[0;36mbuild_weighted_fed_avg\u001b[0;34m(model_fn, client_optimizer_fn, server_optimizer_fn, client_weighting, model_distributor, model_aggregator, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    221\u001b[0m   client_work \u001b[39m=\u001b[39m model_delta_client_work\u001b[39m.\u001b[39mbuild_functional_model_delta_client_work(\n\u001b[1;32m    222\u001b[0m       model\u001b[39m=\u001b[39mmodel_fn,\n\u001b[1;32m    223\u001b[0m       optimizer\u001b[39m=\u001b[39mclient_optimizer_fn,\n\u001b[1;32m    224\u001b[0m       client_weighting\u001b[39m=\u001b[39mclient_weighting,\n\u001b[1;32m    225\u001b[0m       metrics_aggregator\u001b[39m=\u001b[39mmetrics_aggregator)\n\u001b[1;32m    226\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m   client_work \u001b[39m=\u001b[39m model_delta_client_work\u001b[39m.\u001b[39;49mbuild_model_delta_client_work(\n\u001b[1;32m    228\u001b[0m       model_fn\u001b[39m=\u001b[39;49mmodel_fn,\n\u001b[1;32m    229\u001b[0m       optimizer\u001b[39m=\u001b[39;49mclient_optimizer_fn,\n\u001b[1;32m    230\u001b[0m       client_weighting\u001b[39m=\u001b[39;49mclient_weighting,\n\u001b[1;32m    231\u001b[0m       metrics_aggregator\u001b[39m=\u001b[39;49mmetrics_aggregator,\n\u001b[1;32m    232\u001b[0m       use_experimental_simulation_loop\u001b[39m=\u001b[39;49muse_experimental_simulation_loop)\n\u001b[1;32m    233\u001b[0m finalizer \u001b[39m=\u001b[39m apply_optimizer_finalizer\u001b[39m.\u001b[39mbuild_apply_optimizer_finalizer(\n\u001b[1;32m    234\u001b[0m     server_optimizer_fn, model_weights_type)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m composers\u001b[39m.\u001b[39mcompose_learning_process(initial_model_weights_fn,\n\u001b[1;32m    236\u001b[0m                                           model_distributor, client_work,\n\u001b[1;32m    237\u001b[0m                                           aggregator, finalizer)\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:356\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work\u001b[0;34m(model_fn, optimizer, client_weighting, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    352\u001b[0m get_hparams_fn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    353\u001b[0m set_hparams_fn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m@tensorflow_computation\u001b[39;49m\u001b[39m.\u001b[39;49mtf_computation(weights_type, data_type)\n\u001b[0;32m--> 356\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mclient_update_computation\u001b[39;49m(initial_model_weights, dataset):\n\u001b[1;32m    357\u001b[0m   keras_optimizer \u001b[39m=\u001b[39;49m optimizer()\n\u001b[1;32m    358\u001b[0m   client_update \u001b[39m=\u001b[39;49m build_model_delta_update_with_keras_optimizer(\n\u001b[1;32m    359\u001b[0m       model_fn\u001b[39m=\u001b[39;49mmodel_fn,\n\u001b[1;32m    360\u001b[0m       weighting\u001b[39m=\u001b[39;49mclient_weighting,\n\u001b[1;32m    361\u001b[0m       use_experimental_simulation_loop\u001b[39m=\u001b[39;49muse_experimental_simulation_loop)\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:497\u001b[0m, in \u001b[0;36mComputationWrapper.__call__\u001b[0;34m(self, tff_internal_types, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m   \u001b[39m# Either we have a concrete parameter type, or this is no-arg function.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m   parameter_type \u001b[39m=\u001b[39m _parameter_type(parameters, parameter_types)\n\u001b[0;32m--> 497\u001b[0m   wrapped_func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy(\n\u001b[1;32m    498\u001b[0m       fn_to_wrap, fn_name, parameter_type, unpack\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    500\u001b[0m \u001b[39m# Copy the __doc__ attribute with the documentation in triple-quotes from\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# the decorated function.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m wrapped_func\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fn_to_wrap, \u001b[39m'\u001b[39m\u001b[39m__doc__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:224\u001b[0m, in \u001b[0;36mPythonTracingStrategy.__call__\u001b[0;34m(self, fn_to_wrap, fn_name, parameter_type, unpack)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m   args, kwargs \u001b[39m=\u001b[39m unpack_arguments_fn(packed_args)\n\u001b[0;32m--> 224\u001b[0m   result \u001b[39m=\u001b[39m fn_to_wrap(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    225\u001b[0m   \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[39mraise\u001b[39;00m ComputationReturnedNoneError(fn_to_wrap)\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:362\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work.<locals>.client_update_computation\u001b[0;34m(initial_model_weights, dataset)\u001b[0m\n\u001b[1;32m    357\u001b[0m keras_optimizer \u001b[39m=\u001b[39m optimizer()\n\u001b[1;32m    358\u001b[0m client_update \u001b[39m=\u001b[39m build_model_delta_update_with_keras_optimizer(\n\u001b[1;32m    359\u001b[0m     model_fn\u001b[39m=\u001b[39mmodel_fn,\n\u001b[1;32m    360\u001b[0m     weighting\u001b[39m=\u001b[39mclient_weighting,\n\u001b[1;32m    361\u001b[0m     use_experimental_simulation_loop\u001b[39m=\u001b[39muse_experimental_simulation_loop)\n\u001b[0;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m client_update(keras_optimizer, initial_model_weights, dataset)\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/tmp/__autograph_generated_filet1r0u7ra.py:65\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update\u001b[0;34m(optimizer, initial_weights, data)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[39mreturn\u001b[39;00m fscope_2\u001b[39m.\u001b[39mret(retval__2, do_return_2)\n\u001b[0;32m---> 65\u001b[0m num_examples \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(dataset_reduce_fn), (ag__\u001b[39m.\u001b[39mld(reduce_fn), ag__\u001b[39m.\u001b[39mld(data)), \u001b[39mdict\u001b[39m(initial_state_fn\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(initial_state_for_reduce_fn)), fscope)\n\u001b[1;32m     66\u001b[0m client_update \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure, (ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39msubtract, ag__\u001b[39m.\u001b[39mld(initial_weights)\u001b[39m.\u001b[39mtrainable, ag__\u001b[39m.\u001b[39mld(model_weights)\u001b[39m.\u001b[39mtrainable), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     67\u001b[0m model_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mreport_local_unfinalized_metrics, (), \u001b[39mNone\u001b[39;00m, fscope)\n",
            "File \u001b[0;32m/tmp/__autograph_generated_file3j1tropf.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___dataset_reduce_fn\u001b[0;34m(reduce_fn, dataset, initial_state_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(dataset)\u001b[39m.\u001b[39mreduce, (), \u001b[39mdict\u001b[39m(initial_state\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(initial_state_fn), (), \u001b[39mNone\u001b[39;00m, fscope), reduce_func\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(reduce_fn)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/tmp/__autograph_generated_filet1r0u7ra.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update.<locals>.reduce_fn\u001b[0;34m(num_examples_sum, batch)\u001b[0m\n\u001b[1;32m     21\u001b[0m retval__1 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     22\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 23\u001b[0m     output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mforward_pass, (ag__\u001b[39m.\u001b[39mld(batch),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope_1)\n\u001b[1;32m     24\u001b[0m gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(output)\u001b[39m.\u001b[39mloss, ag__\u001b[39m.\u001b[39mld(model_weights)\u001b[39m.\u001b[39mtrainable), \u001b[39mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     25\u001b[0m grads_and_vars \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(gradients), ag__\u001b[39m.\u001b[39mld(model_weights)\u001b[39m.\u001b[39mtrainable), \u001b[39mNone\u001b[39;00m, fscope_1)\n",
            "File \u001b[0;32m/tmp/__autograph_generated_filevmtf7mtw.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_forward_pass, (ag__\u001b[39m.\u001b[39mld(batch_input),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/tmp/__autograph_generated_file439xdvr2.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(inputs) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, if_body_1, else_body_1, get_state_1, set_state_1, (), \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m predictions \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpredict_on_batch, (ag__\u001b[39m.\u001b[39mld(inputs), ag__\u001b[39m.\u001b[39mld(training)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m (y_true,)\n",
            "File \u001b[0;32m/tmp/__autograph_generated_filerm_e4igu.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_on_batch\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_keras_model, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/keras/engine/input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[0;32m--> 295\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    297\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 175, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 33, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 458, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 411, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 401, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/onos/FL/venv/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 1, 129), found shape=(None, 129)\n"
          ]
        }
      ],
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(\n",
        "        learning_rate=client_lr),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(\n",
        "        learning_rate=server_lr)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state = iterative_process.initialize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "current_time = now.strftime(\"%y_%m_%d_%H\")\n",
        "print(\"Current Time =\", current_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_manager = FileCheckpointManager(\n",
        "    model_dir/current_time/\"{}_{}\".format(experiment_name,method))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time()\n",
        "for i in range(NUM_ROUNDS):\n",
        "    # Train\n",
        "    result = iterative_process.next(state, train_data)\n",
        "    state = result.state\n",
        "    train_metrics = result.metrics['client_work']['train']\n",
        "\n",
        "    # Validation\n",
        "    federated_metrics = evaluator(result.state.global_model_weights, val_data)\n",
        "    val_metrics = federated_metrics['eval']\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = train_metrics['loss']\n",
        "    train_acc = train_metrics['sparse_categorical_accuracy']\n",
        "    val_loss = val_metrics['loss']\n",
        "    val_acc = val_metrics['sparse_categorical_accuracy']\n",
        "\n",
        "    # Print\n",
        "    print('round {:2d}\\ntrain_loss={l:.3f}, train_acc={ac:.3f}'.format(\n",
        "        i+1, l=train_loss, ac=train_metrics['sparse_categorical_accuracy']))\n",
        "    print('val_loss: {:.3f} val_acc: {:.3f}'.format(\n",
        "        val_loss, val_acc))\n",
        "\n",
        "    # Save\n",
        "    ckpt_manager.save_checkpoint(state, round_num=i)\n",
        "    # logs\n",
        "    tff_train_acc.append(float(train_metrics['sparse_categorical_accuracy']))\n",
        "    tff_train_loss.append(float(train_metrics['loss']))\n",
        "    tff_val_acc.append(float(val_metrics['sparse_categorical_accuracy']))\n",
        "    tff_val_loss.append(float(val_metrics['loss']))\n",
        "\n",
        "end = time() - start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "federated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_time = \"Time: {}\".format(sec_to_hours(end))\n",
        "print(total_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_val = str(round(train_acc*100)) + \"_\" + str(round(val_acc*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "this_dir = Path.cwd()\n",
        "model_dir = this_dir / \"sdn_saved_models\" / experiment_name / method / train_val\n",
        "output_dir = this_dir / \"sdn_results\" / experiment_name / method / train_val\n",
        "\n",
        "if not model_dir.exists():\n",
        "    model_dir.mkdir(parents=True)\n",
        "\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir(parents=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOBx91fpdAFs"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dir/'parameters.txt', 'w') as f:\n",
        "    print('client_lr: {}\\nserver_lr: {}\\nRounds: {}\\nBATCH_SIZE: {}'.format(\n",
        "        client_lr, server_lr, NUM_ROUNDS, BATCH_SIZE), file=f)\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_file = open(output_dir/\"time.txt\", \"w\")\n",
        "n = text_file.write(total_time)\n",
        "text_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_for_inference = create_keras_model(NUM_CLASSES)\n",
        "predictions = model_for_inference.predict_on_batch(x_test)\n",
        "y_pred = np.argmax(predictions, axis=-1)\n",
        "classes = []\n",
        "for c in range(NUM_CLASSES):\n",
        "    classes.append(\"Class {}\".format(c))\n",
        "print(classes)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "with open(output_dir/'metrics.txt', 'w') as f:\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "    print('Confusion Matrix\\n', file=f)\n",
        "    print(confusion, file = f)\n",
        "\n",
        "    # importing accuracy_score, precision_score, recall_score, f1_score\n",
        "    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)), file=f)\n",
        "\n",
        "    print('Micro Precision: {:.2f}'.format(\n",
        "        precision_score(y_test, y_pred, average='micro')), file=f)\n",
        "    print('Micro Recall: {:.2f}'.format(\n",
        "        recall_score(y_test, y_pred, average='micro')), file=f)\n",
        "    print(\n",
        "        'Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')), file=f)\n",
        "\n",
        "    print('Macro Precision: {:.2f}'.format(\n",
        "        precision_score(y_test, y_pred, average='macro')), file=f)\n",
        "    print('Macro Recall: {:.2f}'.format(\n",
        "        recall_score(y_test, y_pred, average='macro')), file=f)\n",
        "    print(\n",
        "        'Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')), file=f)\n",
        "\n",
        "    print('Weighted Precision: {:.2f}'.format(\n",
        "        precision_score(y_test, y_pred, average='weighted')), file=f)\n",
        "    print('Weighted Recall: {:.2f}'.format(\n",
        "        recall_score(y_test, y_pred, average='weighted')), file=f)\n",
        "    print(\n",
        "        'Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')), file=f)\n",
        "\n",
        "    print('\\nClassification Report\\n', file = f)\n",
        "    print(classification_report(y_test, y_pred, target_names=classes), file=f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "plot_graph(list(range(0, NUM_ROUNDS)), tff_train_acc, label='Train Accuracy')\n",
        "plot_graph(list(range(0, NUM_ROUNDS)), tff_val_acc,\n",
        "           label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / \"federated_model_Accuracy.png\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_graph(list(range(0, NUM_ROUNDS)), tff_train_loss, label='Train loss')\n",
        "plot_graph(list(range(0, NUM_ROUNDS)), tff_val_loss, label='Validation loss')\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / \"federated_model_loss.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnAXQMx_dAFs"
      },
      "source": [
        "## Single Model with all Data at once (for comparison)\n",
        "\n",
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqL-buDmdAFt"
      },
      "outputs": [],
      "source": [
        "# train_data = train_data[0].concatenate(train_data[1])\n",
        "# val_data = val_data[0].concatenate(val_data[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDjQc-u0dAFt"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj7Wsal0dAFt"
      },
      "outputs": [],
      "source": [
        "# def model_fn():\n",
        "#     model = tf.keras.models.Sequential([\n",
        "#         tf.keras.layers.InputLayer(input_shape=(4,)),\n",
        "#         tf.keras.layers.Dense(32, activation='relu'),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "#         tf.keras.layers.Dense(32, activation='relu'),\n",
        "#         tf.keras.layers.Dense(3, activation='sigmoid'),\n",
        "#     ])\n",
        "\n",
        "#     model.compile(\n",
        "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "#         optimizer=tf.keras.optimizers.Adam(),\n",
        "#         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        "#     )\n",
        "\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDYXpu6WdAFt"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm8nwUyDdAFt"
      },
      "outputs": [],
      "source": [
        "# model = model_fn()\n",
        "# history = model.fit(train_data, epochs=NUM_ROUNDS,verbose=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ_J02EAdAFt"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_xbTiDpdAFu"
      },
      "outputs": [],
      "source": [
        "# test_scores = model.evaluate(val_data)\n",
        "# single_metrics = {\n",
        "#     'loss': test_scores[0],\n",
        "#     'accuracy': test_scores[1],\n",
        "# }\n",
        "# single_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfJ1_ak4dAFu"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Comparing both models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp2npXnodAFu"
      },
      "outputs": [],
      "source": [
        "# print(f\"---Single model metrics---\\n{single_metrics}\\n\")\n",
        "# print(f\"---Federated model metrics---\\n{dict(federated_metrics)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jgEX9PadAFu"
      },
      "source": [
        "The Federated Learning approach has a better balance between precision and recall, which might be an indicator of better handling of the imbalanced dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14 (main, Sep 18 2022, 16:34:37) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "addd2fdd290c7c34336629330a81969ca1164c4689498c0841c953bddca49006"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
