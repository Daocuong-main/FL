{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bZAnYwAkdAFl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "from time import time, localtime\n",
        "import numpy as np\n",
        "from utils import plot_graph\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras import losses, metrics, optimizers\n",
        "import random\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "from pathlib import Path\n",
        "from checkpoint_manager import FileCheckpointManager\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from keras.layers import TimeDistributed, Conv1D, MaxPool1D, Flatten, LSTM, Dense, AveragePooling1D\n",
        "from keras.models import Sequential\n",
        "from keras import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.set_printoptions(threshold=sys.maxsize) \n",
        "nest_asyncio.apply()\n",
        "SEED = 1337\n",
        "tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def train_test_split(df, frac=0.2):\n",
        "#     selected = df['flow_id'].drop_duplicates().sample(frac=frac)\n",
        "#     test = df[df['flow_id'].isin(selected)]\n",
        "#     train = df[~df['flow_id'].isin(selected)]\n",
        "#     return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def test_case_split(df,split):\n",
        "#     selected = df['flow_id'].drop_duplicates().sample(frac=1)\n",
        "#     # print(selected.shape)\n",
        "#     total_data_count = selected.shape[0]\n",
        "#     data_per_set = int(np.floor(total_data_count/split))\n",
        "#     DataFrameDict = {}\n",
        "#     for i in range(1, split+1):\n",
        "#         client_name = \"client_\" + str(i)\n",
        "#         start = data_per_set * (i-1)\n",
        "#         end = data_per_set * i\n",
        "\n",
        "#         print(f\"Adding data from {start} to {end} for client : {client_name}\")\n",
        "#         DataFrameDict[client_name] = df[df['flow_id'].isin(selected[start:end])]\n",
        "#     return DataFrameDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lựa chọn model <br>\n",
        "Thay models.<Ten_model>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.CNN_2D import create_keras_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thay đổi các siêu tham số trong cell dưới <br>\n",
        "**experiment_name:** Tên bộ dữ liệu được sử dụng, <Tên dữ liệu>_<Số byte sử dụng> <br>\n",
        "**method:** Mô hình sử dụng <br>\n",
        "**client_lr, server_lr:** Learning rate của client và server, khi fine tune bắt đầu từ 1 và giảm dần xuống 3e-4 <br>\n",
        "**NUM_ROUNDS:** Số vòng lặp (Bắt đầu từ 1 và tăng dần lên 3000 nếu mô hình chưa hội tụ - 1/100/200/300/400/500/.../3000) <Br>\n",
        "**BATCH_SIZE:** Kích thước batch (Bắt đầu từ 8 và tăng dần lên 64) 8/16/32/64 <br>\n",
        "**split:** Để nguyên là 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "byte_number (string): Cac byte cua packet, bao gom 10, 32, 64, 128, 256, 512, 1024, 1460"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "byte_number = \"512\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZYYi42tYdAFp"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"GQUIC_\" + byte_number\n",
        "method = \"CNN_2D\"\n",
        "client_lr = 3e-4\n",
        "server_lr = 3e-4\n",
        "NUM_ROUNDS = 100\n",
        "BATCH_SIZE = 32\n",
        "split = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "this_dir = Path.cwd()\n",
        "model_dir = this_dir / \"sdn_saved_models\" / experiment_name / method\n",
        "output_dir = this_dir / \"sdn_results\" / experiment_name / method\n",
        "\n",
        "if not model_dir.exists():\n",
        "    model_dir.mkdir(parents=True)\n",
        "\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir(parents=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sec_to_hours(seconds):\n",
        "    a = seconds//3600\n",
        "    b = (seconds % 3600)//60\n",
        "    c = (seconds % 3600) % 60\n",
        "    d = \"{:.0f} hours {:.0f} mins {:.0f} seconds\".format(a, b, c)\n",
        "    return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_frequent(List):\n",
        "    return max(set(List), key=List.count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Đọc dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(np.unique(train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28 10\n"
          ]
        }
      ],
      "source": [
        "NUM_FEATURE = train_images.shape[-1]\n",
        "NUM_CLASSES = len(np.unique(train_labels))\n",
        "print(NUM_FEATURE, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "client_1_x, client_1_y, client_2_x, client_2_y = train_test_split(train_images,train_labels,test_size=0.5,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpgncmHIdAFp"
      },
      "source": [
        "## Federated Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIg-Ji57dAFp"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIx3CIRydAFq"
      },
      "outputs": [],
      "source": [
        "def make_tf_dataset(dataframe, batch_size=None):\n",
        "\n",
        "    result = dataframe.groupby('flow_id')['Label'].apply(list).to_dict()\n",
        "    y = []\n",
        "    for flow in result:\n",
        "        y.append(most_frequent(result[flow]))\n",
        "    y = np.array(y)\n",
        "    dataframe = dataframe.drop(['Label','flow_id'], axis=1).to_numpy()\n",
        "    dataframe = dataframe.reshape(-1, 20, NUM_FEATURE,1)\n",
        "    # Dataset creation\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dataframe, y.reshape(-1,1)))\n",
        "    if batch_size:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBj7fQxFdAFq"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = [], []\n",
        "for client_data in DataFrameDict.keys():\n",
        "    train_df, val_df = train_test_split(DataFrameDict[client_data], frac=0.2)\n",
        "    # TF Datasets\n",
        "    train_data.append(make_tf_dataset(train_df, batch_size=BATCH_SIZE))\n",
        "    val_data.append(make_tf_dataset(val_df, batch_size=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP8byNHDdAFq"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCci5e2-dAFq"
      },
      "outputs": [],
      "source": [
        "def input_spec():\n",
        "    return (\n",
        "        tf.TensorSpec([None, 20, NUM_FEATURE,1], tf.int64),\n",
        "        tf.TensorSpec([None, 1], tf.int64)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    model = create_keras_model(NUM_FEATURE, NUM_CLASSES)\n",
        "\n",
        "    return tff.learning.from_keras_model(\n",
        "        model,\n",
        "        input_spec=input_spec(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd99tw3RdAFr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XUqmO4HdAFs"
      },
      "source": [
        "Each time the `next` method is called, the server model is broadcast to each client using a broadcast function. For each client, one epoch of local training is performed. Each client computes the difference between the client model after training and the initial broadcast model. These model deltas are then aggregated at the server using some aggregation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tff_train_acc = []\n",
        "tff_train_loss = []\n",
        "tff_val_acc = []\n",
        "tff_val_loss = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1YqmgzhdAFr"
      },
      "outputs": [],
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(\n",
        "        learning_rate=client_lr),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(\n",
        "        learning_rate=server_lr)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator = tff.learning.build_federated_evaluation(model_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state = iterative_process.initialize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "current_time = now.strftime(\"%y_%m_%d_%H_%M\")\n",
        "print(\"Current Time =\", current_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_round = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ckpt_manager = FileCheckpointManager(\n",
        "#     model_dir/\"22_10_31_02_09\"/\"{}_{}\".format(experiment_name, method))\n",
        "# state,current_round = ckpt_manager.load_latest_checkpoint(state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_manager = FileCheckpointManager(\n",
        "    model_dir/current_time/\"{}_{}\".format(experiment_name, method), keep_total=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not current_round:\n",
        "    current_round = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time()\n",
        "for i in range(current_round, NUM_ROUNDS):\n",
        "    # Train\n",
        "    result = iterative_process.next(state, train_data)\n",
        "    state = result.state\n",
        "    train_metrics = result.metrics['client_work']['train']\n",
        "\n",
        "    # Validation\n",
        "    federated_metrics = evaluator(result.state.global_model_weights, val_data)\n",
        "    val_metrics = federated_metrics['eval']\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = train_metrics['loss']\n",
        "    train_acc = train_metrics['sparse_categorical_accuracy']\n",
        "    val_loss = val_metrics['loss']\n",
        "    val_acc = val_metrics['sparse_categorical_accuracy']\n",
        "\n",
        "    # Print\n",
        "    print('round {:2d}\\ntrain_loss={l:.3f}, train_acc={ac:.3f}'.format(\n",
        "        i+1, l=train_loss, ac=train_metrics['sparse_categorical_accuracy']))\n",
        "    print('val_loss: {:.3f} val_acc: {:.3f}'.format(\n",
        "        val_loss, val_acc))\n",
        "\n",
        "    # Save\n",
        "    ckpt_manager.save_checkpoint(state, round_num=i)\n",
        "    # logs\n",
        "    tff_train_acc.append(float(train_metrics['sparse_categorical_accuracy']))\n",
        "    tff_train_loss.append(float(train_metrics['loss']))\n",
        "    tff_val_acc.append(float(val_metrics['sparse_categorical_accuracy']))\n",
        "    tff_val_loss.append(float(val_metrics['loss']))\n",
        "    current_round = i\n",
        "\n",
        "end = time() - start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_time = \"Time: {}\".format(sec_to_hours(end))\n",
        "print(total_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_val = str(round(train_acc*100)) + \"_\" + str(round(val_acc*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "this_dir = Path.cwd()\n",
        "model_dir = this_dir / \"sdn_saved_models\" / experiment_name / method / train_val\n",
        "output_dir = this_dir / \"sdn_results\" / experiment_name / method / train_val\n",
        "\n",
        "if not model_dir.exists():\n",
        "    model_dir.mkdir(parents=True)\n",
        "\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir(parents=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOBx91fpdAFs"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_manager = FileCheckpointManager(model_dir)\n",
        "try:\n",
        "    ckpt_manager.save_checkpoint(state, round_num=current_round)\n",
        "except:\n",
        "    print(\"File exist\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dir/'parameters.txt', 'w') as f:\n",
        "    print('client_lr: {}\\nserver_lr: {}\\nRounds: {}\\nBATCH_SIZE: {}'.format(\n",
        "        client_lr, server_lr, NUM_ROUNDS, BATCH_SIZE), file=f)\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_file = open(output_dir/\"time.txt\", \"w\")\n",
        "n = text_file.write(total_time)\n",
        "text_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_for_inference = create_keras_model(NUM_FEATURE, NUM_CLASSES)\n",
        "state.global_model_weights.assign_weights_to(model_for_inference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model_for_inference.predict(x_test, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flow_pred = np.argmax(predictions, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_for_inference.save(model_dir/'model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "classes = []\n",
        "for c in range(NUM_CLASSES):\n",
        "    classes.append(\"Class {}\".format(c))\n",
        "print(classes)\n",
        "with open(output_dir/'metrics.txt', 'w') as f:\n",
        "    confusion = confusion_matrix(flow_label, flow_pred)\n",
        "    print('Confusion Matrix\\n', file=f)\n",
        "    print(confusion, file=f)\n",
        "\n",
        "    # importing accuracy_score, precision_score, recall_score, f1_score\n",
        "    print('\\nAccuracy: {:.2f}\\n'.format(\n",
        "        accuracy_score(flow_label, flow_pred)), file=f)\n",
        "\n",
        "    print('Micro Precision: {:.2f}'.format(\n",
        "        precision_score(flow_label, flow_pred, average='micro')), file=f)\n",
        "    print('Micro Recall: {:.2f}'.format(\n",
        "        recall_score(flow_label, flow_pred, average='micro')), file=f)\n",
        "    print(\n",
        "        'Micro F1-score: {:.2f}\\n'.format(f1_score(flow_label, flow_pred, average='micro')), file=f)\n",
        "\n",
        "    print('Macro Precision: {:.2f}'.format(\n",
        "        precision_score(flow_label, flow_pred, average='macro')), file=f)\n",
        "    print('Macro Recall: {:.2f}'.format(\n",
        "        recall_score(flow_label, flow_pred, average='macro')), file=f)\n",
        "    print(\n",
        "        'Macro F1-score: {:.2f}\\n'.format(f1_score(flow_label, flow_pred, average='macro')), file=f)\n",
        "\n",
        "    print('Weighted Precision: {:.2f}'.format(\n",
        "        precision_score(flow_label, flow_pred, average='weighted')), file=f)\n",
        "    print('Weighted Recall: {:.2f}'.format(\n",
        "        recall_score(flow_label, flow_pred, average='weighted')), file=f)\n",
        "    print(\n",
        "        'Weighted F1-score: {:.2f}'.format(f1_score(flow_label, flow_pred, average='weighted')), file=f)\n",
        "\n",
        "    print('\\nClassification Report\\n', file=f)\n",
        "    print(classification_report(flow_label, flow_pred, target_names=classes), file=f)\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "plot_graph(list(range(0, current_round+1)),\n",
        "           tff_train_acc, label='Train Accuracy')\n",
        "plot_graph(list(range(0, current_round+1)), tff_val_acc,\n",
        "           label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / \"federated_model_Accuracy.png\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_graph(list(range(0, current_round+1)), tff_train_loss, label='Train loss')\n",
        "plot_graph(list(range(0, current_round+1)),\n",
        "           tff_val_loss, label='Validation loss')\n",
        "plt.legend()\n",
        "plt.savefig(output_dir / \"federated_model_loss.png\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "addd2fdd290c7c34336629330a81969ca1164c4689498c0841c953bddca49006"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
