{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bZAnYwAkdAFl"
      },
      "outputs": [],
      "source": [
        "from time import time, localtime\n",
        "import numpy as np\n",
        "from utils import plot_graph\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import losses, metrics, optimizers\n",
        "import random\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "from pathlib import Path\n",
        "from checkpoint_manager import FileCheckpointManager\n",
        "import tensorflow_addons as tfa\n",
        "nest_asyncio.apply()\n",
        "SEED = 1337\n",
        "tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lựa chọn model <br>\n",
        "Thay models.<Ten_model>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.CNNv5 import create_keras_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thay đổi các siêu tham số trong cell dưới <br>\n",
        "**experiment_name:** Tên bộ dữ liệu được sử dụng, <Tên dữ liệu>_<Số byte sử dụng> <br>\n",
        "**method:** Mô hình sử dụng <br>\n",
        "**client_lr, server_lr:** Learning rate của client và server, khi fine tune bắt đầu từ 1 và giảm dần xuống 3e-4 <br>\n",
        "**NUM_ROUNDS:** Số vòng lặp (Bắt đầu từ 1 và tăng dần lên 3000 nếu mô hình chưa hội tụ - 1/100/200/300/400/500/.../3000) <Br>\n",
        "**BATCH_SIZE:** Kích thước batch (Bắt đầu từ 8 và tăng dần lên 64) 8/16/32/64 <br>\n",
        "**split:** Để nguyên là 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "byte_number (string): Cac byte cua packet, bao gom 10, 32, 64, 128, 256, 512, 1024, 1460"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "byte_number = \"512\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZYYi42tYdAFp"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"GQUIC_\" + byte_number\n",
        "method = \"nonFL_CNNv5_with_length\"\n",
        "client_lr = 1e-3\n",
        "NUM_ROUNDS = 100\n",
        "BATCH_SIZE = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "this_dir = Path.cwd()\n",
        "model_dir = this_dir / \"sdn_saved_models\" / experiment_name / method\n",
        "output_dir = this_dir / \"sdn_results\" / experiment_name / method\n",
        "\n",
        "if not model_dir.exists():\n",
        "    model_dir.mkdir(parents=True)\n",
        "\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir(parents=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sec_to_hours(seconds):\n",
        "    a = seconds//3600\n",
        "    b = (seconds % 3600)//60\n",
        "    c = (seconds % 3600) % 60\n",
        "    d = \"{:.0f} hours {:.0f} mins {:.0f} seconds\".format(a, b, c)\n",
        "    return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_frequent(List):\n",
        "    return max(set(List), key=List.count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Đọc dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dir = '/home/onos/FL/Data Processing/GQUIC_data_' + byte_number + '.csv'\n",
        "test_dir = '/home/onos/FL/Data Processing/GQUIC_test_' + byte_number + '.csv'\n",
        "data = pd.read_csv(train_dir, engine='pyarrow')\n",
        "test = pd.read_csv(test_dir, engine='pyarrow')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data.drop(['length'],axis=1,inplace=True)\n",
        "# test.drop(['length'],axis=1,inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "result = test.groupby('flow_id')['Label'].apply(list).to_dict()\n",
        "flow_label = []\n",
        "for flow in result:\n",
        "    flow_label.append(most_frequent(result[flow]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_test = test.drop('flow_id', axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "513 4\n"
          ]
        }
      ],
      "source": [
        "NUM_FEATURE = len(true_test.columns)-1\n",
        "NUM_CLASSES = len(np.unique(true_test['Label']))\n",
        "print(NUM_FEATURE, NUM_CLASSES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test = true_test['Label'].to_numpy()\n",
        "x_test = true_test.drop('Label', axis=1).to_numpy()\n",
        "y_train = data['Label'].to_numpy()\n",
        "x_train = data.drop(['Label'], axis=1).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def sklearn_to_df(sklearn_dataset):\n",
        "#     df = pd.DataFrame(sklearn_dataset.data,\n",
        "#                       columns=sklearn_dataset.feature_names)\n",
        "#     df['Label'] = pd.Series(sklearn_dataset.Label)\n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn import datasets\n",
        "# df = sklearn_to_df(datasets.load_iris())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yqL-buDmdAFt"
      },
      "outputs": [],
      "source": [
        "# train_data = train_data[0].concatenate(train_data[1])\n",
        "# val_data = val_data[0].concatenate(val_data[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDjQc-u0dAFt"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oj7Wsal0dAFt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_2 (Conv1D)           (None, 513, 32)           832       \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 171, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 171, 64)           51264     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 57, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3648)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              3736576   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,792,772\n",
            "Trainable params: 3,792,772\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_keras_model(NUM_FEATURE, NUM_CLASSES)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8529/8529 [==============================] - 518s 61ms/step - loss: 1.0235 - sparse_categorical_accuracy: 0.5169\n",
            "Epoch 2/100\n",
            "8529/8529 [==============================] - 450s 53ms/step - loss: 0.9951 - sparse_categorical_accuracy: 0.5321\n",
            "Epoch 3/100\n",
            "8529/8529 [==============================] - 541s 63ms/step - loss: 0.9892 - sparse_categorical_accuracy: 0.5349\n",
            "Epoch 4/100\n",
            "8529/8529 [==============================] - 436s 51ms/step - loss: 0.9859 - sparse_categorical_accuracy: 0.5362\n",
            "Epoch 5/100\n",
            "8529/8529 [==============================] - 528s 62ms/step - loss: 0.9839 - sparse_categorical_accuracy: 0.5371\n",
            "Epoch 6/100\n",
            "8529/8529 [==============================] - 462s 54ms/step - loss: 0.9824 - sparse_categorical_accuracy: 0.5376\n",
            "Epoch 7/100\n",
            "8529/8529 [==============================] - 476s 56ms/step - loss: 0.9811 - sparse_categorical_accuracy: 0.5379\n",
            "Epoch 8/100\n",
            "8529/8529 [==============================] - 511s 60ms/step - loss: 0.9799 - sparse_categorical_accuracy: 0.5386\n",
            "Epoch 9/100\n",
            "8529/8529 [==============================] - 434s 51ms/step - loss: 0.9786 - sparse_categorical_accuracy: 0.5392\n",
            "Epoch 10/100\n",
            "8529/8529 [==============================] - 527s 62ms/step - loss: 0.9775 - sparse_categorical_accuracy: 0.5395\n",
            "Epoch 11/100\n",
            "8529/8529 [==============================] - 453s 53ms/step - loss: 0.9763 - sparse_categorical_accuracy: 0.5403\n",
            "Epoch 12/100\n",
            "8529/8529 [==============================] - 478s 56ms/step - loss: 0.9750 - sparse_categorical_accuracy: 0.5410\n",
            "Epoch 13/100\n",
            "8529/8529 [==============================] - 510s 60ms/step - loss: 0.9737 - sparse_categorical_accuracy: 0.5415\n",
            "Epoch 14/100\n",
            "8529/8529 [==============================] - 490s 57ms/step - loss: 0.9725 - sparse_categorical_accuracy: 0.5421\n",
            "Epoch 15/100\n",
            "8529/8529 [==============================] - 517s 61ms/step - loss: 0.9713 - sparse_categorical_accuracy: 0.5429\n",
            "Epoch 16/100\n",
            "8529/8529 [==============================] - 527s 62ms/step - loss: 0.9699 - sparse_categorical_accuracy: 0.5432\n",
            "Epoch 17/100\n",
            "8529/8529 [==============================] - 488s 57ms/step - loss: 0.9689 - sparse_categorical_accuracy: 0.5436\n",
            "Epoch 18/100\n",
            "8529/8529 [==============================] - 578s 68ms/step - loss: 0.9676 - sparse_categorical_accuracy: 0.5443\n",
            "Epoch 19/100\n",
            "8529/8529 [==============================] - 490s 57ms/step - loss: 0.9665 - sparse_categorical_accuracy: 0.5448\n",
            "Epoch 20/100\n",
            "8529/8529 [==============================] - 527s 62ms/step - loss: 0.9653 - sparse_categorical_accuracy: 0.5454\n",
            "Epoch 21/100\n",
            "8529/8529 [==============================] - 525s 62ms/step - loss: 0.9643 - sparse_categorical_accuracy: 0.5463\n",
            "Epoch 22/100\n",
            "8529/8529 [==============================] - 489s 57ms/step - loss: 0.9631 - sparse_categorical_accuracy: 0.5467\n",
            "Epoch 23/100\n",
            "8529/8529 [==============================] - 560s 66ms/step - loss: 0.9622 - sparse_categorical_accuracy: 0.5473\n",
            "Epoch 24/100\n",
            "8529/8529 [==============================] - 475s 56ms/step - loss: 0.9615 - sparse_categorical_accuracy: 0.5476\n",
            "Epoch 25/100\n",
            "8529/8529 [==============================] - 562s 66ms/step - loss: 0.9604 - sparse_categorical_accuracy: 0.5480\n",
            "Epoch 26/100\n",
            "8529/8529 [==============================] - 486s 57ms/step - loss: 0.9595 - sparse_categorical_accuracy: 0.5483\n",
            "Epoch 27/100\n",
            "8529/8529 [==============================] - 526s 62ms/step - loss: 0.9586 - sparse_categorical_accuracy: 0.5487\n",
            "Epoch 28/100\n",
            "8529/8529 [==============================] - 532s 62ms/step - loss: 0.9576 - sparse_categorical_accuracy: 0.5490\n",
            "Epoch 29/100\n",
            "8529/8529 [==============================] - 484s 57ms/step - loss: 0.9571 - sparse_categorical_accuracy: 0.5496\n",
            "Epoch 30/100\n",
            "8529/8529 [==============================] - 573s 67ms/step - loss: 0.9560 - sparse_categorical_accuracy: 0.5499\n",
            "Epoch 31/100\n",
            "8529/8529 [==============================] - 475s 56ms/step - loss: 0.9555 - sparse_categorical_accuracy: 0.5503\n",
            "Epoch 32/100\n",
            "8529/8529 [==============================] - 542s 64ms/step - loss: 0.9551 - sparse_categorical_accuracy: 0.5505\n",
            "Epoch 33/100\n",
            "8529/8529 [==============================] - 526s 62ms/step - loss: 0.9544 - sparse_categorical_accuracy: 0.5508\n",
            "Epoch 34/100\n",
            "8529/8529 [==============================] - 498s 58ms/step - loss: 0.9537 - sparse_categorical_accuracy: 0.5512\n",
            "Epoch 35/100\n",
            "8529/8529 [==============================] - 574s 67ms/step - loss: 0.9531 - sparse_categorical_accuracy: 0.5517\n",
            "Epoch 36/100\n",
            "8529/8529 [==============================] - 487s 57ms/step - loss: 0.9528 - sparse_categorical_accuracy: 0.5518\n",
            "Epoch 37/100\n",
            "8529/8529 [==============================] - 541s 63ms/step - loss: 0.9522 - sparse_categorical_accuracy: 0.5520\n",
            "Epoch 38/100\n",
            "8529/8529 [==============================] - 534s 63ms/step - loss: 0.9519 - sparse_categorical_accuracy: 0.5520\n",
            "Epoch 39/100\n",
            "8529/8529 [==============================] - 488s 57ms/step - loss: 0.9515 - sparse_categorical_accuracy: 0.5522\n",
            "Epoch 40/100\n",
            "8529/8529 [==============================] - 573s 67ms/step - loss: 0.9511 - sparse_categorical_accuracy: 0.5523\n",
            "Epoch 41/100\n",
            "8529/8529 [==============================] - 468s 55ms/step - loss: 0.9505 - sparse_categorical_accuracy: 0.5525\n",
            "Epoch 42/100\n",
            "8529/8529 [==============================] - 528s 62ms/step - loss: 0.9501 - sparse_categorical_accuracy: 0.5527\n",
            "Epoch 43/100\n",
            "8529/8529 [==============================] - 451s 53ms/step - loss: 0.9498 - sparse_categorical_accuracy: 0.5530\n",
            "Epoch 44/100\n",
            "8529/8529 [==============================] - 586s 69ms/step - loss: 0.9493 - sparse_categorical_accuracy: 0.5533\n",
            "Epoch 45/100\n",
            "8529/8529 [==============================] - 500s 59ms/step - loss: 0.9495 - sparse_categorical_accuracy: 0.5532\n",
            "Epoch 46/100\n",
            "8529/8529 [==============================] - 548s 64ms/step - loss: 0.9490 - sparse_categorical_accuracy: 0.5535\n",
            "Epoch 47/100\n",
            "8529/8529 [==============================] - 524s 61ms/step - loss: 0.9487 - sparse_categorical_accuracy: 0.5537\n",
            "Epoch 48/100\n",
            "8529/8529 [==============================] - 527s 62ms/step - loss: 0.9485 - sparse_categorical_accuracy: 0.5537\n",
            "Epoch 49/100\n",
            "8529/8529 [==============================] - 578s 68ms/step - loss: 0.9483 - sparse_categorical_accuracy: 0.5537\n",
            "Epoch 50/100\n",
            "8529/8529 [==============================] - 502s 59ms/step - loss: 0.9479 - sparse_categorical_accuracy: 0.5540\n",
            "Epoch 51/100\n",
            "8529/8529 [==============================] - 556s 65ms/step - loss: 0.9474 - sparse_categorical_accuracy: 0.5543\n",
            "Epoch 52/100\n",
            "8529/8529 [==============================] - 526s 62ms/step - loss: 0.9476 - sparse_categorical_accuracy: 0.5543\n",
            "Epoch 53/100\n",
            "8529/8529 [==============================] - 517s 61ms/step - loss: 0.9472 - sparse_categorical_accuracy: 0.5545\n",
            "Epoch 54/100\n",
            "8529/8529 [==============================] - 561s 66ms/step - loss: 0.9472 - sparse_categorical_accuracy: 0.5544\n",
            "Epoch 55/100\n",
            "8529/8529 [==============================] - 488s 57ms/step - loss: 0.9469 - sparse_categorical_accuracy: 0.5545\n",
            "Epoch 56/100\n",
            "8529/8529 [==============================] - 590s 69ms/step - loss: 0.9469 - sparse_categorical_accuracy: 0.5546\n",
            "Epoch 57/100\n",
            "8529/8529 [==============================] - 490s 57ms/step - loss: 0.9469 - sparse_categorical_accuracy: 0.5548\n",
            "Epoch 58/100\n",
            "8529/8529 [==============================] - 558s 65ms/step - loss: 0.9468 - sparse_categorical_accuracy: 0.5547\n",
            "Epoch 59/100\n",
            "8529/8529 [==============================] - 441s 52ms/step - loss: 0.9469 - sparse_categorical_accuracy: 0.5546\n",
            "Epoch 60/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9468 - sparse_categorical_accuracy: 0.5547\n",
            "Epoch 61/100\n",
            "8529/8529 [==============================] - 267s 31ms/step - loss: 0.9467 - sparse_categorical_accuracy: 0.5548\n",
            "Epoch 62/100\n",
            "8529/8529 [==============================] - 266s 31ms/step - loss: 0.9463 - sparse_categorical_accuracy: 0.5549\n",
            "Epoch 63/100\n",
            "8529/8529 [==============================] - 267s 31ms/step - loss: 0.9466 - sparse_categorical_accuracy: 0.5547\n",
            "Epoch 64/100\n",
            "8529/8529 [==============================] - 266s 31ms/step - loss: 0.9466 - sparse_categorical_accuracy: 0.5547\n",
            "Epoch 65/100\n",
            "8529/8529 [==============================] - 261s 31ms/step - loss: 0.9460 - sparse_categorical_accuracy: 0.5550\n",
            "Epoch 66/100\n",
            "8529/8529 [==============================] - 263s 31ms/step - loss: 0.9457 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 67/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9454 - sparse_categorical_accuracy: 0.5553\n",
            "Epoch 68/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9458 - sparse_categorical_accuracy: 0.5552\n",
            "Epoch 69/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9459 - sparse_categorical_accuracy: 0.5551\n",
            "Epoch 70/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9459 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 71/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9454 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 72/100\n",
            "8529/8529 [==============================] - 262s 31ms/step - loss: 0.9450 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 73/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9458 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 74/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9452 - sparse_categorical_accuracy: 0.5557\n",
            "Epoch 75/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9449 - sparse_categorical_accuracy: 0.5559\n",
            "Epoch 76/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9453 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 77/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9455 - sparse_categorical_accuracy: 0.5553\n",
            "Epoch 78/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9453 - sparse_categorical_accuracy: 0.5555\n",
            "Epoch 79/100\n",
            "8529/8529 [==============================] - 262s 31ms/step - loss: 0.9444 - sparse_categorical_accuracy: 0.5559\n",
            "Epoch 80/100\n",
            "8529/8529 [==============================] - 262s 31ms/step - loss: 0.9448 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 81/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9460 - sparse_categorical_accuracy: 0.5551\n",
            "Epoch 82/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9451 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 83/100\n",
            "8529/8529 [==============================] - 266s 31ms/step - loss: 0.9446 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 84/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9450 - sparse_categorical_accuracy: 0.5557\n",
            "Epoch 85/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9448 - sparse_categorical_accuracy: 0.5553\n",
            "Epoch 86/100\n",
            "8529/8529 [==============================] - 262s 31ms/step - loss: 0.9447 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 87/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9448 - sparse_categorical_accuracy: 0.5555\n",
            "Epoch 88/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9453 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 89/100\n",
            "8529/8529 [==============================] - 269s 32ms/step - loss: 0.9445 - sparse_categorical_accuracy: 0.5558\n",
            "Epoch 90/100\n",
            "8529/8529 [==============================] - 267s 31ms/step - loss: 0.9452 - sparse_categorical_accuracy: 0.5557\n",
            "Epoch 91/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9447 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 92/100\n",
            "8529/8529 [==============================] - 266s 31ms/step - loss: 0.9448 - sparse_categorical_accuracy: 0.5553\n",
            "Epoch 93/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9447 - sparse_categorical_accuracy: 0.5555\n",
            "Epoch 94/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9446 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 95/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9450 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 96/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9447 - sparse_categorical_accuracy: 0.5556\n",
            "Epoch 97/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9450 - sparse_categorical_accuracy: 0.5554\n",
            "Epoch 98/100\n",
            "8529/8529 [==============================] - 266s 31ms/step - loss: 0.9443 - sparse_categorical_accuracy: 0.5557\n",
            "Epoch 99/100\n",
            "8529/8529 [==============================] - 265s 31ms/step - loss: 0.9444 - sparse_categorical_accuracy: 0.5559\n",
            "Epoch 100/100\n",
            "8529/8529 [==============================] - 264s 31ms/step - loss: 0.9441 - sparse_categorical_accuracy: 0.5559\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "    learning_rate=client_lr), loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "start = time()\n",
        "history = model.fit(x_train, y_train, epochs=NUM_ROUNDS, batch_size=BATCH_SIZE,use_multiprocessing=True,shuffle=False)\n",
        "end = time() - start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 11 hours 26 mins 31 seconds\n"
          ]
        }
      ],
      "source": [
        "total_time = \"Time: {}\".format(sec_to_hours(end))\n",
        "print(total_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ_J02EAdAFt"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_accuracy = history.history['sparse_categorical_accuracy'][np.argmin(history.history['loss'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2133/2133 - 16s - loss: 1.1924 - sparse_categorical_accuracy: 0.5120 - 16s/epoch - 7ms/step\n"
          ]
        }
      ],
      "source": [
        "_, test_acc = model.evaluate(x_test, y_test,verbose=2, batch_size=BATCH_SIZE,use_multiprocessing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_val = str(round(best_model_accuracy*100)) + \"_\" + str(round(test_acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "this_dir = Path.cwd()\n",
        "model_dir = this_dir / \"sdn_saved_models\" / experiment_name / method / train_val\n",
        "output_dir = this_dir / \"sdn_results\" / experiment_name / method / train_val\n",
        "\n",
        "if not model_dir.exists():\n",
        "    model_dir.mkdir(parents=True)\n",
        "\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir/'model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dir/'parameters.txt', 'w') as f:\n",
        "    print('client_lr: {}\\nEpochs: {}\\nBATCH_SIZE: {}'.format(\n",
        "        client_lr, NUM_ROUNDS, BATCH_SIZE), file=f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_file = open(output_dir/\"time.txt\", \"w\")\n",
        "n = text_file.write(total_time)\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2133/2133 - 16s - 16s/epoch - 7ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test,verbose=2,use_multiprocessing=True,batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = np.argmax(predictions, axis=-1)\n",
        "test['pred'] = y_pred.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = test.groupby('flow_id')['pred'].apply(list).to_dict()\n",
        "flow_pred = []\n",
        "for flow in result:\n",
        "    flow_pred.append(most_frequent(result[flow]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Class 0', 'Class 1', 'Class 2', 'Class 3']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "classes = []\n",
        "for c in range(NUM_CLASSES):\n",
        "    classes.append(\"Class {}\".format(c))\n",
        "print(classes)\n",
        "with open(output_dir/'metrics.txt', 'w') as f:\n",
        "    confusion = confusion_matrix(flow_label, flow_pred)\n",
        "    print('Confusion Matrix\\n', file=f)\n",
        "    print(confusion, file=f)\n",
        "\n",
        "    # importing accuracy_score, precision_score, recall_score, f1_score\n",
        "    print('\\nAccuracy: {:.2f}\\n'.format(\n",
        "        accuracy_score(flow_label, flow_pred)), file=f)\n",
        "\n",
        "    print('Micro Precision: {:.2f}'.format(\n",
        "        precision_score(flow_label, flow_pred, average='micro')), file=f)\n",
        "    print('Micro Recall: {:.2f}'.format(\n",
        "        recall_score(flow_label, flow_pred, average='micro')), file=f)\n",
        "    print(\n",
        "        'Micro F1-score: {:.2f}\\n'.format(f1_score(flow_label, flow_pred, average='micro')), file=f)\n",
        "\n",
        "    print('Macro Precision: {:.2f}'.format(\n",
        "        precision_score(flow_label, flow_pred, average='macro')), file=f)\n",
        "    print('Macro Recall: {:.2f}'.format(\n",
        "        recall_score(flow_label, flow_pred, average='macro')), file=f)\n",
        "    print(\n",
        "        'Macro F1-score: {:.2f}\\n'.format(f1_score(flow_label, flow_pred, average='macro')), file=f)\n",
        "\n",
        "    print('Weighted Precision: {:.2f}'.format(\n",
        "        precision_score(flow_label, flow_pred, average='weighted')), file=f)\n",
        "    print('Weighted Recall: {:.2f}'.format(\n",
        "        recall_score(flow_label, flow_pred, average='weighted')), file=f)\n",
        "    print(\n",
        "        'Weighted F1-score: {:.2f}'.format(f1_score(flow_label, flow_pred, average='weighted')), file=f)\n",
        "\n",
        "    print('\\nClassification Report\\n', file=f)\n",
        "    print(classification_report(flow_label, flow_pred, target_names=classes), file=f)\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0,1])\n",
        "plt.savefig(output_dir / \"normal_model_Accuracy.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42644.8"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape[0]/20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10661.2"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'flow_id'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'flow_id'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data[\u001b[39m'\u001b[39;49m\u001b[39mflow_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mnunique()\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/FL/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'flow_id'"
          ]
        }
      ],
      "source": [
        "data['flow_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flow_id</th>\n",
              "      <th>length</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>...</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>Label</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.143820</td>\n",
              "      <td>0.878431</td>\n",
              "      <td>0.203922</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.094118</td>\n",
              "      <td>0.545098</td>\n",
              "      <td>0.313725</td>\n",
              "      <td>0.145098</td>\n",
              "      <td>0.556863</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.321569</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.443137</td>\n",
              "      <td>0.486275</td>\n",
              "      <td>0.188235</td>\n",
              "      <td>0.709804</td>\n",
              "      <td>0.152941</td>\n",
              "      <td>0.925490</td>\n",
              "      <td>...</td>\n",
              "      <td>0.525490</td>\n",
              "      <td>0.474510</td>\n",
              "      <td>0.078431</td>\n",
              "      <td>0.090196</td>\n",
              "      <td>0.211765</td>\n",
              "      <td>0.403922</td>\n",
              "      <td>0.349020</td>\n",
              "      <td>0.584314</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.010487</td>\n",
              "      <td>0.215686</td>\n",
              "      <td>0.427451</td>\n",
              "      <td>0.870588</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.137255</td>\n",
              "      <td>0.921569</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.010487</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.611765</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.427451</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>0.137255</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.603922</td>\n",
              "      <td>0.243137</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>0.057678</td>\n",
              "      <td>0.149020</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.250980</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>0.439216</td>\n",
              "      <td>0.949020</td>\n",
              "      <td>0.125490</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.443137</td>\n",
              "      <td>0.992157</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.305882</td>\n",
              "      <td>0.196078</td>\n",
              "      <td>0.870588</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>0.121348</td>\n",
              "      <td>0.474510</td>\n",
              "      <td>0.556863</td>\n",
              "      <td>0.909804</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.341176</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.054902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>0.013483</td>\n",
              "      <td>0.278431</td>\n",
              "      <td>0.211765</td>\n",
              "      <td>0.301961</td>\n",
              "      <td>0.149020</td>\n",
              "      <td>0.623529</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.576471</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.917647</td>\n",
              "      <td>0.572549</td>\n",
              "      <td>0.415686</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.239216</td>\n",
              "      <td>0.329412</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.901961</td>\n",
              "      <td>0.737255</td>\n",
              "      <td>0.168627</td>\n",
              "      <td>0.262745</td>\n",
              "      <td>0.945098</td>\n",
              "      <td>0.356863</td>\n",
              "      <td>0.560784</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.380392</td>\n",
              "      <td>0.478431</td>\n",
              "      <td>0.325490</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.956863</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.380392</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5</td>\n",
              "      <td>0.021723</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.035294</td>\n",
              "      <td>0.396078</td>\n",
              "      <td>0.407843</td>\n",
              "      <td>0.250980</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.964706</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>6</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.274510</td>\n",
              "      <td>0.274510</td>\n",
              "      <td>0.188235</td>\n",
              "      <td>0.564706</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>0.654902</td>\n",
              "      <td>0.694118</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>0.850980</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.423529</td>\n",
              "      <td>0.552941</td>\n",
              "      <td>0.607843</td>\n",
              "      <td>0.243137</td>\n",
              "      <td>0.572549</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>6</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.929412</td>\n",
              "      <td>0.650980</td>\n",
              "      <td>0.403922</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.670588</td>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.090196</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>8</td>\n",
              "      <td>0.021723</td>\n",
              "      <td>0.627451</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.525490</td>\n",
              "      <td>0.239216</td>\n",
              "      <td>0.858824</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.215686</td>\n",
              "      <td>0.376471</td>\n",
              "      <td>0.917647</td>\n",
              "      <td>0.976471</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.360784</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>9</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.603922</td>\n",
              "      <td>0.572549</td>\n",
              "      <td>0.729412</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.545098</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>9</td>\n",
              "      <td>0.017228</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.886275</td>\n",
              "      <td>0.403922</td>\n",
              "      <td>0.023529</td>\n",
              "      <td>0.250980</td>\n",
              "      <td>0.090196</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.419608</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10</td>\n",
              "      <td>0.109363</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.643137</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.192157</td>\n",
              "      <td>0.380392</td>\n",
              "      <td>0.301961</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.286275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>10</td>\n",
              "      <td>0.012734</td>\n",
              "      <td>0.760784</td>\n",
              "      <td>0.301961</td>\n",
              "      <td>0.498039</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.780392</td>\n",
              "      <td>0.694118</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.921569</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 516 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    flow_id    length         0         1         2         3         4  \\\n",
              "0         1  0.143820  0.878431  0.203922  0.003922  0.094118  0.545098   \n",
              "1         2  1.000000  0.321569  0.819608  0.443137  0.486275  0.188235   \n",
              "2         2  0.010487  0.215686  0.427451  0.870588  0.070588  0.137255   \n",
              "3         3  0.010487  0.266667  0.611765  0.678431  0.427451  0.827451   \n",
              "4         3  0.017228  0.011765  0.588235  0.333333  0.317647  0.600000   \n",
              "5         3  0.057678  0.149020  0.941176  0.600000  0.250980  0.023529   \n",
              "6         3  0.017228  0.443137  0.992157  0.666667  0.305882  0.196078   \n",
              "7         4  0.121348  0.474510  0.556863  0.909804  0.866667  0.341176   \n",
              "8         4  0.013483  0.278431  0.211765  0.301961  0.149020  0.623529   \n",
              "9         4  0.017228  0.917647  0.572549  0.415686  0.011765  0.176471   \n",
              "10        5  1.000000  0.901961  0.737255  0.168627  0.262745  0.945098   \n",
              "11        5  0.021723  0.627451  0.035294  0.396078  0.407843  0.250980   \n",
              "12        6  1.000000  0.862745  0.274510  0.274510  0.188235  0.564706   \n",
              "13        6  0.017228  0.929412  0.650980  0.403922  0.294118  0.670588   \n",
              "14        8  0.021723  0.627451  0.800000  0.831373  0.098039  0.525490   \n",
              "15        8  0.017228  0.215686  0.376471  0.917647  0.976471  0.776471   \n",
              "16        9  0.017228  0.352941  0.682353  0.725490  0.603922  0.572549   \n",
              "17        9  0.017228  0.760784  0.886275  0.403922  0.023529  0.250980   \n",
              "18       10  0.109363  0.725490  0.643137  0.705882  0.192157  0.380392   \n",
              "19       10  0.012734  0.760784  0.301961  0.498039  0.725490  0.780392   \n",
              "\n",
              "           5         6         7  ...       504       505       506       507  \\\n",
              "0   0.313725  0.145098  0.556863  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "1   0.709804  0.152941  0.925490  ...  0.525490  0.474510  0.078431  0.090196   \n",
              "2   0.921569  0.674510  0.800000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "3   0.498039  0.137255  0.678431  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "4   0.603922  0.243137  0.819608  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "5   0.439216  0.949020  0.125490  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "6   0.870588  0.098039  0.941176  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "7   0.509804  0.796078  0.054902  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "8   0.568627  0.576471  0.800000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "9   0.066667  0.239216  0.329412  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "10  0.356863  0.560784  0.831373  ...  0.380392  0.478431  0.325490  0.047059   \n",
              "11  0.498039  0.690196  0.964706  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "12  0.007843  0.654902  0.694118  ...  0.039216  0.850980  0.317647  0.423529   \n",
              "13  0.815686  0.090196  0.933333  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "14  0.239216  0.858824  0.533333  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "15  1.000000  0.690196  0.360784  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "16  0.729412  0.117647  0.545098  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "17  0.090196  0.760784  0.419608  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "18  0.301961  0.600000  0.286275  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "19  0.694118  0.294118  0.921569  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "         508       509       510       511  Label  pred  \n",
              "0   0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "1   0.211765  0.403922  0.349020  0.584314      0     3  \n",
              "2   0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "3   0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "4   0.000000  0.000000  0.000000  0.000000      0     3  \n",
              "5   0.000000  0.000000  0.000000  0.000000      0     3  \n",
              "6   0.000000  0.000000  0.000000  0.000000      0     3  \n",
              "7   0.000000  0.000000  0.000000  0.000000      3     3  \n",
              "8   0.000000  0.000000  0.000000  0.000000      3     3  \n",
              "9   0.000000  0.000000  0.000000  0.000000      3     3  \n",
              "10  0.956863  0.815686  0.380392  0.509804      0     3  \n",
              "11  0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "12  0.552941  0.607843  0.243137  0.572549      0     3  \n",
              "13  0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "14  0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "15  0.000000  0.000000  0.000000  0.000000      0     3  \n",
              "16  0.000000  0.000000  0.000000  0.000000      0     3  \n",
              "17  0.000000  0.000000  0.000000  0.000000      0     3  \n",
              "18  0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "19  0.000000  0.000000  0.000000  0.000000      0     0  \n",
              "\n",
              "[20 rows x 516 columns]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "76009"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test['flow_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14 (main, Sep 18 2022, 16:34:37) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "addd2fdd290c7c34336629330a81969ca1164c4689498c0841c953bddca49006"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
